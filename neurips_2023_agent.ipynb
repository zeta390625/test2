{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "960679152249533",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# NeurIPS 2023问答agent（未完成）\n",
    "+ 主要特点\n",
    "    + 跨领域、文档的全面分析\n",
    "    + 相关内容的理解和定位\n",
    "+ 例子：\n",
    "    > “基于GNN的多跳推理在这次会议上有新的进展吗？”\n",
    "    > “因果推理用于改善决策的问题有什么新思路吗？”\n",
    "\n",
    "说明：    \n",
    "+ 需要安装redis\n",
    "+ 用了clash进行本地代理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d639d6be897a51c9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 数据采集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T05:26:34.417856559Z",
     "start_time": "2024-10-22T05:26:32.182784731Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# NeurIPS 2023的论文标题列表\n",
    "import re, requests, aiohttp, asyncio, os, pickle, json, time, redis, re, random, hashlib\n",
    "from fn import F\n",
    "from returns.future import Future\n",
    "from tqdm.asyncio import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "os.environ['http_proxy'] = \"http://127.0.0.1:7890\"\n",
    "os.environ['https_proxy'] = \"http://127.0.0.1:7890\"\n",
    "\n",
    "neurips_url = 'https://neurips.cc/virtual/2023/papers.html?filter=titles'\n",
    "titles = re.findall(r'<a href=\"[^\"]*\">(.+?)</a>', requests.get(neurips_url).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7cc7159f654534",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 并发API获取和存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95ad315a74a191f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T05:27:24.449672079Z",
     "start_time": "2024-10-22T05:27:24.312082329Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import httpx\n",
    "\n",
    "redis_client = redis.Redis(db=15)\n",
    "\n",
    "\n",
    "# 一个通用异步网络请求框架\n",
    "# fetch逻辑是把data参数传给invoker然后获取response。invoker可以是任意的请求调用函数\n",
    "\n",
    "\n",
    "def build_fetcher(invoker):\n",
    "    ...\n",
    "    async def fetch(**args):\n",
    "        # async with semaphore:\n",
    "        assert 'semaphore' in args.keys()\n",
    "        async with args['semaphore']:\n",
    "            try:\n",
    "                async with await invoker(**args) as response: #coroutine\n",
    "                    if response.status == 200:\n",
    "                        res = await response.text()\n",
    "                        assert 'data' in args.keys()\n",
    "                        return {'input':args['data'], 'response':res}\n",
    "                    else:\n",
    "                        error_message = await response.text()\n",
    "                        print(f\"Request failed: {response.status}. Error message: {error_message}\")\n",
    "            except Exception as e:\n",
    "                print(f\"error: {e}\")\n",
    "    fetch.url = invoker.url\n",
    "    return fetch\n",
    "\n",
    "# 并发调用fetch&然后缓存到redis的一种实现方式。基于输入的data用key模板函数分配唯一的key。\n",
    "# data → fetch → (key, result) → redis\n",
    "# 会给invoker的每个协程分配session和同步信号量\n",
    "\n",
    "def fetch_spawn_concurrent(fetcher, key_template, workers=10):\n",
    "    async def _fetch_spawn_concurrent(inputs):\n",
    "        \n",
    "        key_set = set(map(key_template, inputs))\n",
    "        print('total:', len(key_set))\n",
    "        print('to fetch: ', len(set(filter(lambda x:not redis_client.exists(x), key_set))))\n",
    "        \n",
    "        semaphore = asyncio.Semaphore(workers)\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "        # async with httpx.AsyncClient(timeout=5,\n",
    "        #                              proxies={\"http://\": \"http://127.0.0.1:7890\", \n",
    "        #                                       \"https://\": \"http://127.0.0.1:7890\"}\n",
    "        #                              ) as invoker_client:\n",
    "            tasks = [fetcher(data=input, \n",
    "                             semaphore=semaphore, \n",
    "                             session=session) for input in inputs \n",
    "                     if not redis_client.exists(key_template(input))]\n",
    "            for f in tqdm.as_completed(tasks, total=len(tasks), desc=f'[Fetching] {fetcher.url}'):\n",
    "                result = await f\n",
    "                match result:\n",
    "                    case {'input':input, 'response':_}:\n",
    "                        redis_client.set(key_template(input), json.dumps(result))\n",
    "    return _fetch_spawn_concurrent\n",
    "\n",
    "# 用于元数据的redis key模板函数\n",
    "# data → key\n",
    "def key_template_metadata(source, conf):\n",
    "    def _key_template(data):\n",
    "        _data = re.sub('[^\\w\\-_\\s]', '', re.sub(':', '-', data))\n",
    "        return f\"{source}:{conf}:{_data}\"\n",
    "    return _key_template\n",
    "\n",
    "# 定义了采集不同数据的redis key模板函数\n",
    "key_template_neurips_abs = key_template_metadata('neurips:abs', 'NeurIPS-2023')\n",
    "key_template_arxiv = key_template_metadata('arxiv', 'NeurIPS-2023')\n",
    "key_template_openreview = key_template_metadata('openreview', 'NeurIPS-2023')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e0b0f6ccb31348",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 获取官方元数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89d9f080e4a17167",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T05:28:07.405145677Z",
     "start_time": "2024-10-22T05:28:03.821471674Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 3540\n",
      "to fetch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fetching] https://papers.nips.cc/paper_files: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "os.environ['http_proxy'] = \"http://127.0.0.1:7890\"\n",
    "os.environ['https_proxy'] = \"http://127.0.0.1:7890\"\n",
    "\n",
    "# 流程是根据title获取abstract的相对路径，然后拼成完整路径用fetch_spawn_concurrent并发调用\n",
    "\n",
    "abs_urls, titles_ = zip(*re.findall(r'<a title=\"paper title\" href=\"([^\"]*)\">(.+?)</a>', \n",
    "                                    requests.get('https://papers.nips.cc/paper_files/paper/2023').text))\n",
    "\n",
    "abs_urls = list(map(lambda x:x.replace('_files/paper', ''), abs_urls))\n",
    "# title到abs相对路径的映射\n",
    "title2abs = dict(zip(titles_, abs_urls))\n",
    "\n",
    "NeurIPS_ABS_API_URL = 'https://papers.nips.cc/paper_files'\n",
    "\n",
    "# async def invoker_neurips_abs(client, data, **kargs):\n",
    "#     return client.stream('GET', url=NeurIPS_ABS_API_URL+title2abs[data])\n",
    "\n",
    "#只需要定义调用请求的invoker和存在redis的key模板函数，就可以用并发函数执行了\n",
    "\n",
    "async def invoker_neurips_abs(session, data, **kargs):\n",
    "    return session.get(url=NeurIPS_ABS_API_URL+title2abs[data],\n",
    "                       proxy=\"http://127.0.0.1:7890\")\n",
    "\n",
    "invoker_neurips_abs.url = NeurIPS_ABS_API_URL\n",
    "fetcher_neurips_abs = build_fetcher(invoker_neurips_abs)\n",
    "\n",
    "await fetch_spawn_concurrent(fetcher_neurips_abs, key_template_neurips_abs, workers=10)(titles_)\n",
    "\n",
    "# 单个用例的情况：\n",
    "# semaphore = asyncio.Semaphore(1)\n",
    "# async with aiohttp.ClientSession() as session:\n",
    "#     z = await fetcher_neurips_abs(titles[0], semaphore, session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40bbdbaf703a482",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 随机删掉十条记录然后测试上面的并发调用\n",
    "[redis_client.delete(x) for x in random.sample(redis_client.keys('neurips*'), 10)]\n",
    "!rma -s localhost -p 6379 -d 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e541e63b005d62",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## arxiv、openreview元数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d85a79bd36042d36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T05:27:35.694987966Z",
     "start_time": "2024-10-22T05:27:35.253737632Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 3584\n",
      "to fetch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fetching] http://export.arxiv.org/api/query: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# 原理和官方元数据的调用基本相同。请求的参数会有区别\n",
    "\n",
    "ARXIV_API_URL = \"http://export.arxiv.org/api/query\"\n",
    "OPENREVIEW_API_URL = \"https://api2.openreview.net/notes/search\"\n",
    "\n",
    "\n",
    "async def invoker_arxiv(session, data, **kargs):\n",
    "    return session.get(url=ARXIV_API_URL, \n",
    "                       proxy=\"http://127.0.0.1:7890\", \n",
    "                       params={'search_query':f'ti:{data}', 'start':0, 'max_results':1})\n",
    "invoker_arxiv.url = ARXIV_API_URL\n",
    "\n",
    "\n",
    "async def invoker_openreview(session, data, **kargs):\n",
    "    return session.get(url=invoker_openreview.url, \n",
    "                       proxy=\"http://127.0.0.1:7890\", \n",
    "                       params={\"term\": data, \"offset\": 0, \"limit\": 1})\n",
    "invoker_openreview.url = OPENREVIEW_API_URL\n",
    "\n",
    "fetcher_arxiv = build_fetcher(invoker_arxiv)\n",
    "fetcher_openreview = build_fetcher(invoker_openreview)\n",
    "\n",
    "await fetch_spawn_concurrent(fetcher_arxiv, key_template_arxiv, workers=10)(titles)\n",
    "# await fetch_spawn_concurrent(fetcher_openreview, key_template_openreview, workers=1)(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bbadf19b7d896",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#同样是随机删掉十条记录用于测试\n",
    "[redis_client.delete(x) for x in random.sample(redis_client.keys('arxiv*'), 10)]\n",
    "# [redis_client.delete(x) for x in redis_client.keys('research_topic*')]\n",
    "!rma -s localhost -p 6379 -d 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3587db44d8f8c9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据清洗、处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f64562a9417ea10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T05:27:42.113577969Z",
     "start_time": "2024-10-22T05:27:42.107492113Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from returns.context import Reader\n",
    "from returns.unsafe import unsafe_perform_io\n",
    "from returns.maybe import Maybe, Nothing\n",
    "\n",
    "# 定义了一个Maybe的monad chain用于元数据后处理。输入：title，输出：采集的内容。\n",
    "# Maybe chain用于处理每一步可能出现的空值异常。暂时直接过滤了没有填充缺失值\n",
    "\n",
    "metadata_chain = lambda key_template: lambda x: (((Maybe.from_optional(x)\n",
    "                                         .bind_optional(key_template))\n",
    "                                         .bind_optional(lambda x:redis_client.get(x)))\n",
    "                                         # .or_else_call(...)\n",
    "                                         # .alt(...)\n",
    "                                         .bind_optional(lambda x:x.decode('utf-8'))\n",
    "                                         .bind_optional(lambda x:json.loads(x).get('response')))\n",
    "\n",
    "post_process = lambda metadata: {x[0]:x[1].unwrap() for x in filter(lambda x:x[1]!=Nothing, metadata)}\n",
    "\n",
    "def mapper():\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74f0245ba50cd14d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T05:27:46.615295322Z",
     "start_time": "2024-10-22T05:27:45.280479081Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xmltodict\n",
    "from difflib import SequenceMatcher\n",
    "from returns.pipeline import flow\n",
    "\n",
    "# arxiv_chain = lambda x:((metadata_chain(x, key_template_arxiv)\n",
    "#                         .bind_optional(xmltodict.parse))\n",
    "#                         .bind_optional(lambda x:x.get('feed'))\n",
    "#                         .bind_optional(lambda x:x.get('entry'))\n",
    "#                         # .or_else_call({'entry': x}))\n",
    "#                         .bind_optional(lambda x:{'entry':x, 'arxiv_title':x.get('title')}))\n",
    "# \n",
    "# \n",
    "# metadata_arxiv = list(map(lambda x: (x, arxiv_chain(x)), titles))\n",
    "\n",
    "#用于解析arxiv请求结果的chain。输入：采集的响应数据，输出：元数据json。\n",
    "\n",
    "arxiv_chain = lambda x:((Maybe.from_optional(x)\n",
    "                        .bind_optional(xmltodict.parse))\n",
    "                        .bind_optional(lambda x:x.get('feed'))\n",
    "                        .bind_optional(lambda x:x.get('entry'))\n",
    "                        # .or_else_call({'entry': x}))\n",
    "                        .bind_optional(lambda x:{'entry':x, 'arxiv_title':x.get('title')}))\n",
    "\n",
    "#将metadata_chain和arxiv_chain拼起来得到title→元数据的chain\n",
    "\n",
    "metadata_arxiv = list(map(lambda x: (x, flow(x, \n",
    "                                             metadata_chain(key_template_arxiv), \n",
    "                                             lambda x:x.unwrap(), \n",
    "                                             arxiv_chain)), titles))\n",
    "\n",
    "metadata_arxiv = post_process(metadata_arxiv)\n",
    "\n",
    "# 过滤query和检索结果的title不一致的情况\n",
    "metadta_arxiv_filter = lambda x:SequenceMatcher(None, x[0], x[1].get('arxiv_title')).ratio() > 0.9\n",
    "metadata_arxiv = dict(filter(metadta_arxiv_filter, metadata_arxiv.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4173286064d4802",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T05:33:09.362851702Z",
     "start_time": "2024-10-22T05:33:08.899828859Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 官方元数据解析的chain。title→元数据\n",
    "metadata_neurips_abs = list(map(lambda x:\n",
    "                                (x,metadata_chain(key_template_neurips_abs)(x)\n",
    "                                 .bind_optional(lambda x: re.findall(r'<h4>Abstract</h4>\\s*<p>(?:<p>)?([\\s\\S]*?)<\\/p>',x))\n",
    "                                 ),\n",
    "                                titles))\n",
    "\n",
    "metadata_neurips_abs = post_process(metadata_neurips_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "10bbc88b47df3b01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T03:56:13.419565857Z",
     "start_time": "2024-10-21T03:56:11.388404163Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import anyio\n",
    "import asyncio\n",
    "from functools import partial\n",
    "\n",
    "# 这里说明了独立定义这些chain的好处。可以进行各种业务逻辑的形式化组合。\n",
    "# 比如说用future(一种monad)在arxiv检索结果和query不一致的时候用其他方式获取数据\n",
    "\n",
    "async def fetch_chain(title):\n",
    "    semaphore = asyncio.Semaphore(10)\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        \n",
    "        fetcher_arxiv_ = partial(fetcher_arxiv, semaphore=semaphore, session=session)\n",
    "        result = await (Future.from_value(title)\n",
    "                        .bind_awaitable(fetcher_arxiv_)\n",
    "                        .map(lambda x:x.get('response'))\n",
    "                        .map(arxiv_chain)\n",
    "                        .map(...)\n",
    "                        .awaitable())\n",
    "        return result\n",
    "\n",
    "z = await asyncio.get_event_loop().run_in_executor(None, anyio.run, fetch_chain)\n",
    "z1 = unsafe_perform_io(z).unwrap()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33a1fbee7b1f1d9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 基于Claude的问答数据集合成\n",
    "+ 用Claude总结所有的研究主题。输入：标题列表，输出：一组主题名称\n",
    "+ 为每个研究主题构造一些问题\n",
    "+ 用Retriever获取问题的相关论文\n",
    "+ 用Claude评估问题和论文的相关性\n",
    "+ 相关性用于训练RAG模型\n",
    "# RAG pipeline\n",
    "+ 基于Bi-encoder的初排（语义相似度）\n",
    "+ 基于Cross-encoder的重排（阅读理解）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ae40599e66ba07",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 总结NeurIPS 2023的主题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "2d62188731ab5b16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T17:16:45.419295844Z",
     "start_time": "2024-10-20T17:16:45.375792513Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# titles_id = list(enumerate(metadata_neurips_abs.keys()))\n",
    "# id2title = dict(titles_id)\n",
    "# ids = id2title.keys()\n",
    "# titles = list(id2title.values())\n",
    "# abs_text = list(map(lambda x:metadata_neurips_abs[x][0], titles))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8d5aa21ab167af",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 基于标题列表生成主题名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "65e57393e7c10ad6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T06:34:07.382259775Z",
     "start_time": "2024-10-22T06:34:07.363910261Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "# 把完整标题列表分块调用Claude生成主题，最后再合并结果\n",
    "\n",
    "chunk_sz = 400\n",
    "\n",
    "prompt_topic_gen = \"\"\"\n",
    "Task: Analyze and categorize paper titles from NeurIPS 2023\n",
    "\n",
    "Dataset: 3500 paper titles, to be processed in batches of {{chunk_sz}}\n",
    "\n",
    "For each batch, perform the following:\n",
    "\n",
    "1. Identify coarse-grained research topics. The research topics should not be defined too broadly (such as machine learning), but need to have a certain degree of distinction (e.g., In-context learning, Graph Contrastive Learning, etc.)\n",
    "2. Associate each paper with the identified areas and topics\n",
    "\n",
    "Input format: List of paper titles with corresponding IDs\n",
    "\n",
    "Output format: JSON structure as follows:\n",
    "\n",
    "  \"research_topics\": {\n",
    "    \"topic1\": [\"paper_id1\", \"paper_id2\", ...],\n",
    "    \"topic2\": [...],\n",
    "    ...\n",
    "  }\n",
    "}\n",
    "\n",
    "Batch 1 of paper titles:\n",
    "\n",
    "{{titles}}\n",
    "\n",
    "Provide the analysis for this batch. Subsequent batches will build upon the areas and topics identified in previous iterations.\n",
    "\"\"\".replace('{{chunk_sz}}', str(chunk_sz))\n",
    "\n",
    "key_list = list(enumerate(metadata_neurips_abs.keys()))\n",
    "\n",
    "# 为每个块生成输入的context\n",
    "contexts_topic = list(map(lambda x: ((Maybe.from_optional(x)\n",
    "                               .bind_optional(lambda x:json.dumps(key_list[x*chunk_sz:(x+1)*chunk_sz])))\n",
    "                              .bind_optional(lambda x: prompt_topic_gen.replace('{{titles}}', x))\n",
    "                              .unwrap()),\n",
    "                   range(len(metadata_neurips_abs)//chunk_sz+1)))\n",
    "\n",
    "# claude api的并发调用\n",
    "\n",
    "token = os.environ['CLAUDE_API_token']\n",
    "model = \"claude-3-5-sonnet-20240620\"\n",
    "\n",
    "# CLAUDE_API_URL = 'https://api.anthropic.com/v1/messages'\n",
    "CLAUDE_API_URL = 'https://api.gptapi.us/v1/chat/completions' # 某个更稳定的国内CLAUDE中转\n",
    "\n",
    "\n",
    "async def invoker_claude(session, data, **kargs):\n",
    "    return session.post(url=CLAUDE_API_URL,\n",
    "                        headers={'Content-Type': 'application/json',\n",
    "                                 'Authorization': f'Bearer {token}'},\n",
    "                        json={\"model\": model,\n",
    "                                 \"max_tokens\": 4096,\n",
    "                                 \"messages\": [{\"role\": \"user\", \"content\": data}]})\n",
    "\n",
    "invoker_claude.url = CLAUDE_API_URL\n",
    "fetcher_claude = build_fetcher(invoker_claude)\n",
    "redis_client.delete(\"research_topic:claude\")\n",
    "\n",
    "# 对于每个输入context用base64编码作为redis的key\n",
    "key_template_topic = lambda prompt: f\"research_topic:claude:{base64.b64encode(prompt.encode()).decode()}\"\n",
    "\n",
    "# await fetch_spawn_concurrent(fetcher_claude, key_template_topic, workers=10)(contexts_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934d61cf5ca3a794",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[redis_client.delete(x) for x in redis_client.keys('research_topic*')]\n",
    "# [redis_client.delete(x) for x in redis_client.keys('question*')]\n",
    "!rma -s localhost -p 6379 -d 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "292b1a0078cf558d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T06:10:20.698933857Z",
     "start_time": "2024-10-22T06:10:20.658106590Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Claude结果后处理\n",
    "claude_chain = lambda x, key_template: (Maybe.from_optional(x)\n",
    "                                        .bind_optional(key_template)\n",
    "                                        .bind_optional(lambda x:redis_client.get(x))\n",
    "                                        .bind_optional(lambda x:x.decode('utf-8'))\n",
    "                                        .bind_optional(lambda x:json.loads(x)['response'])\n",
    "                                        .bind_optional(json.loads)\n",
    "                                        .bind_optional(lambda x:x['choices'][0]['message']['content'])\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e664d9ccb0c7f1b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 主题合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4dfe711df944d231",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T06:30:40.696321870Z",
     "start_time": "2024-10-22T06:30:40.678207188Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import chain, groupby\n",
    "\n",
    "topic2papers = list(map(lambda x:\n",
    "                       claude_chain(x, key_template_topic)\n",
    "                       .bind_optional(lambda x: re.findall(r'{[^{]*?}', x))\n",
    "                       .bind_optional(lambda x:x[0])\n",
    "                       .bind_optional(lambda x: json.loads(x))\n",
    "                       .unwrap(),\n",
    "                       contexts_topic))\n",
    "\n",
    "topic2papers_flatten = list(chain(*chain(*map(lambda x:\n",
    "                                            [[(a,c) for c in b] for a,b in x.items()], topic2papers))) )\n",
    "\n",
    "topic2papers = {x:list([z[1] for z in y]) for x,y in groupby(\n",
    "    sorted(topic2papers_flatten, key=lambda x:x[0]), key=lambda x:x[0]\n",
    ")}\n",
    "# topic2title = {x:[id2title[int(z)] for z in y] for x,y in topic2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d24a39ecff7689",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 为每个主题生成问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "694b3a3afd5be28b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T06:35:14.805680052Z",
     "start_time": "2024-10-22T06:34:52.044673967Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 10\n",
      "to fetch:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fetching] https://api.gptapi.us/v1/chat/completions: 100%|██████████| 10/10 [00:22<00:00,  2.28s/it]\n"
     ]
    }
   ],
   "source": [
    "prompt_question_gen = \"\"\"\n",
    "You are now taking on the role of an AI researcher.\n",
    "There is a LLM-based question-answering agent specifically designed for NeurIPS 2023. It can answer questions about the academic content of NeurIPS 2023 based on RAG technology.\n",
    "\n",
    "Now, consider the following research topics and their corresponding paper titles:\n",
    "{{topics}}\n",
    "\n",
    "Your task is:\n",
    "From the perspective of researchers in each of the above topics, conceive 5-7 questions related to NeurIPS 2023 based on these research topics. These questions should help you understand the trends in the mentioned research areas at this conference or be helpful for your research. The questions need to be clear, specific, practically meaningful, and in-depth. Please ensure the questions are diverse, including but not limited to:\n",
    "- Analysis of research trends\n",
    "- Discussion of technical details\n",
    "- Comparison with previous years\n",
    "- Cross-domain applications\n",
    "- Current challenges\n",
    "- Future research directions\n",
    "\n",
    "Return the results in the following JSON format:\n",
    "{\n",
    "  \"Topic1\": [\n",
    "    \"Question1\",\n",
    "    \"Question2\",\n",
    "    ...\n",
    "  ],\n",
    "  \"Topic2\": [\n",
    "    \"Question1\",\n",
    "    \"Question2\",\n",
    "    ...\n",
    "  ],\n",
    "  ...\n",
    "}\n",
    "\n",
    "Examples of questions:\n",
    "1. \"Which GNN-related topics received significant attention at this conference?\"\n",
    "2. \"Were there any new developments in GNN-based multi-hop reasoning presented at this conference?\"\n",
    "3. \"Compared to last year's NeurIPS, in which application areas has GNN shown notable expansion?\"\n",
    "4. \"What new discoveries were presented at this conference regarding the combination of GNNs and large language models?\"\n",
    "5. \"What are the main challenges GNNs still face when dealing with dynamic graph structures?\"\n",
    "\"\"\"\n",
    "\n",
    "chunk_sz_question = 10\n",
    "\n",
    "key_list2 = list(enumerate(topic2papers.keys()))\n",
    "\n",
    "prompts_question = list(map(lambda x: (Maybe.from_optional(x)\n",
    "                                       .bind_optional(lambda x:json.dumps(key_list2[x*chunk_sz_question:(x+1)*chunk_sz_question]))\n",
    "                                       .bind_optional(lambda x: prompt_question_gen.replace('{{topics}}', x))\n",
    "                                       .unwrap()),\n",
    "                   range(len(key_list2)//chunk_sz_question+1)))\n",
    "\n",
    "key_template_question = lambda prompt: f\"question:claude:{base64.b64encode(prompt.encode()).decode()}\"\n",
    "\n",
    "await fetch_spawn_concurrent(fetcher_claude, key_template_question, workers=10)(prompts_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "25e0ed719364b5f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T06:38:54.701212689Z",
     "start_time": "2024-10-22T06:38:54.692041450Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic2question = list(map(lambda x:\n",
    "                       claude_chain(x, key_template_question)\n",
    "                       .bind_optional(lambda x: re.findall(r'{[^{]*?}', x))\n",
    "                       .bind_optional(lambda x: x[0])\n",
    "                       .bind_optional(lambda x: json.loads(x))\n",
    "                       .unwrap(),\n",
    "                       prompts_question))\n",
    "\n",
    "questions = list(chain(*chain(*[list(x.values()) for x in topic2question])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd51b1b4c492843f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 论文摘要的embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "acf2892fd1578df0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:34:56.468017391Z",
     "start_time": "2024-10-20T16:34:13.792824995Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name cross-encoder/ms-marco-MiniLM-L-6-v2. Creating a new one with MEAN pooling.\n",
      "100%|██████████| 3512/3512 [00:38<00:00, 90.85it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm as sync_tqdm\n",
    "import numpy as np\n",
    "import torch, faiss\n",
    "\n",
    "biencoder = SentenceTransformer('msmarco-distilbert-base-v4')\n",
    "crossencoder = SentenceTransformer('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = np.array([biencoder.encode(x) for x in sync_tqdm(metadata_neurips_abs.keys())])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac0dadf9f7d3243",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 得到和question相关的候选paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "eeaf2803cbb3b30a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T17:00:22.306446997Z",
     "start_time": "2024-10-20T17:00:22.279286797Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_searcher(embeddings):\n",
    "    index = faiss.IndexFlatL2(len(embeddings[0]))\n",
    "    index.add(embeddings)\n",
    "    def search(query_text, top_k):\n",
    "        query_embedding = biencoder.encode(query_text)\n",
    "        query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "        distances, indices = index.search(query_embedding.reshape(1, -1), top_k)\n",
    "        search.simlarities = 1 - (np.square(distances[0]) / 2)\n",
    "        return indices[0]\n",
    "    return search\n",
    "\n",
    "searcher = build_searcher(embeddings)\n",
    "rs = searcher(questions[0], 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e62d7fbb7c87cd5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T06:39:32.992290732Z",
     "start_time": "2024-10-22T06:39:32.972011499Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "rs = [(q,searcher(q, 10)) for q in sync_tqdm(questions)]\n",
    "retrieve_freq = Counter(list(chain(*rs)))\n",
    "retrieve_freq = {titles[x]:y for x,y in retrieve_freq.items()}\n",
    "# retrieve_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b43ff19198eeca8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO 相关性标签合成：用Claude评估question和初排结果的相关性；biencoder和crossencoder的微调\n",
    "\n",
    "prompt_relv_gen = \"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3810432324c105ef",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
